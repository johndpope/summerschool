{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Bag-of-Words Model\n",
    "\n",
    "In this notebook, we'll move beyond linear classifiers and implement a neural network for our classification task. \n",
    "\n",
    "We'll also introduce the [TensorFlow Estimator API](https://www.tensorflow.org/extend/estimators), which provides a high-level interface similar to scikit-learn. This involves a few new concepts, such as the idea of a `model_fn` and an `input_fn`, but it greatly simplifies experiments and reduces the need to write tedious data-feeding code.\n",
    "\n",
    "## Outline\n",
    "\n",
    "- **Part (a):** Model architecture\n",
    "- **Part (b):** Implementing the Neural BOW model\n",
    "- **Part (c):** Introduction to `tf.Estimator`\n",
    "- **Part (d):** Training, evaluation, and tuning\n",
    "\n",
    "As with the first half of the assignment, exercised are interspersed throughout the notebook. In particular, Part (d) has 4 questions, Part (e) asks you to write code in `models.py`, and Part (f) has 4 questions plus one optional implementation exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Install a few python packages using pip\n",
    "from common import utils\n",
    "utils.require_package(\"wget\")      # for fetching dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iftenney/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding nltk.tree.Tree pretty-printing to use custom GraphViz.\n",
      "Success! All imports done!\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import os, sys, re, json, time, datetime, shutil, copy\n",
    "import itertools, collections\n",
    "from importlib import reload\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# NLTK for NLP utils and corpora\n",
    "import nltk\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "#assert(tf.__version__.startswith(\"1.4\"))\n",
    "\n",
    "# Helper libraries\n",
    "from common import utils, vocabulary, tf_embed_viz, treeviz\n",
    "from common import patched_numpy_io\n",
    "# Code for this assignment\n",
    "import sst, models, models_test\n",
    "\n",
    "# Monkey-patch NLTK with better Tree display that works on Cloud or other display-less server.\n",
    "print(\"Overriding nltk.tree.Tree pretty-printing to use custom GraphViz.\")\n",
    "treeviz.monkey_patch(nltk.tree.Tree, node_style_fn=sst.sst_node_style, format='svg')\n",
    "\n",
    "#test trace\n",
    "print('Success! All imports done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part (a): Model Architecture\n",
    "\n",
    "The neural bag-of-words classifier is one of the simplest neural models for text classification. It takes its name from the bag-of-words assumption common to linear models, in which the weights for each input word are summed to make a prediction. For our neural version, we'll instead sum the _vector representations_ of each word, and then add feed-forward (hidden) layers to make a deep network.\n",
    "\n",
    "Here's a diagram:\n",
    "\n",
    "![Neural Bag-of-Words Model](images/neural_bow.png)\n",
    "\n",
    "We'll use the following notation:\n",
    "- $w^{(i)} \\in \\mathbb{Z}$ for the $i^{th}$ word of the sequence (as an integer index)\n",
    "- $x^{(i)} \\in \\mathbb{R}^d$ for the vector representation (embedding) of $w^{(i)}$\n",
    "- $x \\in \\mathbb{R}^d$ for the fixed-length vector given by summing all the $x^{(i)}$ for an example\n",
    "- $h^{(j)}$ for the hidden state after the $j^{th}$ fully-connected layer\n",
    "- $y$ for the target label ($\\in 1,\\ldots,\\mathtt{num\\_classes}$)\n",
    "\n",
    "Our model is defined as:\n",
    "- **Embedding layer:** $x^{(i)} = W_{embed}[w^{(i)}]$\n",
    "- **Summing vectors:** $x = \\sum_{i=1}^n x^{(i)}$\n",
    "- **Hidden layer(s):** $h^{(j)} = f(h^{(j-1)} W^{(j)} + b^{(j)})$ where $h^{(-1)} = x$ and $j = 0,1,\\ldots,J-1$\n",
    "- **Output layer:** $\\hat{y} = \\hat{P}(y) = \\mathrm{softmax}(h^{(final)} W_{out} + b_{out})$ where $h^{(final)} = h^{(J-1)}$ is the output of the last hidden layer.\n",
    "\n",
    "As per usual, we define the logits to be the argument of the softmax:\n",
    "\n",
    "$$ \\mathrm{logits} = h^{(final)}W_{out} + b_{out} $$\n",
    "\n",
    "We'll refer to the first part of this model (**Embedding layer**, **Summing vectors**, and **Hidden layer(s)**) as the **Encoder**: it has the role of encoding the input sequence into a fixed-length vector representation that we pass to the output layer.\n",
    "\n",
    "We'll also use these as shorthand for important dimensions:\n",
    "- `V`: the vocabulary size (equal to `ds.vocab.size`)\n",
    "- `embed_dim`: the embedding dimension $d$\n",
    "- `hidden_dims`: a list of dimensions for the output of each hidden layer (i.e. $\\mathrm{dim}(h^{(j)})$&nbsp;=&nbsp;`hidden_dims[j]`)\n",
    "- `num_classes`: the number of target classes (2 for the binary task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (a) Exercises\n",
    "\n",
    "Think about the following questions, and work with your group to answer them before moving on to the code in the next section.\n",
    "\n",
    "1. Let `embed_dim = d`, `hidden_dims = [h1, h2]`, and `num_classes = k`. In terms of these values and the vocabulary size `V`, write down the shapes of the following variables: $W_{embed}$, $W^{(0)}$, $b^{(0)}$, $W^{(1)}$, $b^{(1)}$, $W_{out}$, $b_{out}$. (*Hint: $W_{embed}$ has a row for each word in the vocabulary.*)\n",
    "<p>\n",
    "2. Using your answer to 1., how many parameters (matrix or vector elements) are in the embedding layer? How about in the hidden layers? And the output layer?  \n",
    "<p>\n",
    "<p>\n",
    "3. Recall that logistic regression can be thought of as a single-layer neural network. What should we set as the values of `embed_dim` and `hidden_dims` such that this model implements logistic regression?\n",
    "<p>\n",
    "4. Suppose that we have two examples, `[foo bar baz]` and `[baz bar foo]`. Will this model make the same predictions on these? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Minibatches\n",
    "\n",
    "Modern hardware (especially GPUs) performs most efficiently when processing a large amount of data in parallel. Because of this, we usually feed data to a neural network in batches - that is, running several examples at a time, in parallel. If each example is represented by a vector $x \\in \\mathbb{R}^d$, then we can feed in a batch of $m$ examples as a matrix $X \\in \\mathbb{R}^{m \\times d}$, where each row is an example. Note that if we write our matrix-vector products with the vector on the left, as in the equations above, the batch dimension carries through while the rows remain independent:\n",
    "\n",
    "$$ H = f(X W + b) $$\n",
    "\n",
    "is equivalent to computing in parallel $H_i = f(X_i W + b)$ for each $i = 0, \\ldots, m - 1$. Most TensorFlow operations are designed to handle batching seamlessly, so long as $bs$ = `batch_size` is the first dimension of the input data.\n",
    "\n",
    "### Padding Sequences\n",
    "\n",
    "Unlike the Naive Bayes classifier, which took long ($d = V \\approx 16,000$) sparse vectors as input, our neural network will operate directly on a _sequence_ of ids (as stored in `ds.train.ids`). This can be variable-length (depending on the length of the sequence), but we'll need to coerce it into a fixed-length vector for training.\n",
    "\n",
    "The easiest thing to do here is to pad the vectors with a dummy index, which we can zero-out inside our model. Consider the inputs:\n",
    "```\n",
    "[great movies] (2 tokens)\n",
    "[this is a terrible movie] (5 tokens)\n",
    "```\n",
    "We'll convert these to IDs, then pad with a dummy index `0` to get a 2 x 5 matrix:\n",
    "```\n",
    "[[144, 104,  0,   0,  0 ]\n",
    " [ 20,  10,  6, 937, 21]]\n",
    "```\n",
    "\n",
    "For SST, we'll arbitrarily choose to pad to length 40, and clip any examples longer than that. _(Recall from Part (a) that this will only clip fewer than 5% of the dataset.)_\n",
    "\n",
    "The `ds.as_padded_array` function is implemented for you, and will handle clipping and padding automatically. Note the second return value, `*_ns`: this is a vector containing the original (clipped) sequence lengths. We'll use this inside the model to mask the dummy indices so they don't bias our predictions.\n",
    "\n",
    "One last thing: we'll be using [GloVe vectors](https://nlp.stanford.edu/projects/glove/) later, so let's load these now so we can use the same vocabulary. (This will save us from having to re-run the data processing steps later.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vectors from data/glove/glove.6B.zip\n",
      "Parsing file: data/glove/glove.6B.zip:glove.6B.100d.txt\n",
      "Found 400,000 words.\n",
      "Parsing vectors... Done! (W.shape = (400003, 100))\n"
     ]
    }
   ],
   "source": [
    "import glove_helper; reload(glove_helper)\n",
    "\n",
    "hands = glove_helper.Hands(ndim=100)  # 50, 100, 200, 300 dim are available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SST from data/sst/trainDevTestTrees_PTB.zip\n",
      "Training set:     8,544 trees\n",
      "Development set:  1,101 trees\n",
      "Test set:         2,210 trees\n",
      "Using pre-built vocabulary - 400,003 words\n",
      "Processing to phrases...  Done!\n",
      "Splits: train / dev / test : 98,794 / 13,142 / 26,052\n",
      "Success! Stanford Sentiment Treebank (SST Dataset) loaded!\n"
     ]
    }
   ],
   "source": [
    "import sst\n",
    "ds = sst.SSTDataset(V=hands.vocab).process(label_scheme=\"binary\")\n",
    "print('Success! Stanford Sentiment Treebank (SST Dataset) loaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Pre-trained Representations\n",
    "\n",
    "We'll build three versions of the same model: a base model, a version using GloVe embeddings for each word type, and a version using the ELMo language model to provide _contextual_ embeddings for each token. \n",
    "\n",
    "For the first version, we'll learn embeddings from scratch, and so we need to provide the model with lists of token ids. For the latter two, we'll replace the token ids with pre-computed vectors, and train a continuous classifier on top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_len = 40\n",
    "train_x, train_ns, train_y = ds.as_padded_array('train', max_len=max_len, root_only=True)\n",
    "dev_x,   dev_ns,   dev_y   = ds.as_padded_array('dev',   max_len=max_len, root_only=True)\n",
    "test_x,  test_ns,  test_y  = ds.as_padded_array('test',  max_len=max_len, root_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples:\n",
      "\n",
      " [[    3  1140    17 10456     7    33     3  5036   592    12    53    31\n",
      "  18515    30     8    15    21    12   225     7   162    10 16809   154\n",
      "   1416    76  5821  6683     4     2  1464 43710    49  4414 26987     5\n",
      "      0     0     0     0]\n",
      " [    3 78618  5137 10119     6    31     3  2373     6     3  6822    30\n",
      "  12307    17   103  1327    15    10  3238     6  1377    89    39 12426\n",
      "   4469     2  1297  1757    12  2855  3141     6 63556 23465    12 55756\n",
      "      5     0     0     0]\n",
      " [    2  7109  4129 12905    10 15416     6  1502    68    10   309  1159\n",
      "   2045     4    10   309    59  1484 22738     7     3   526    68    37\n",
      "      3  1118  2086  2035 14694     3  2024     4 12601     4  2946     6\n",
      "      3  2368     5     0]]\n",
      "\n",
      "Original sequence lengths: \t [36 37 39]\n",
      "Target labels: \t\t\t [1 1 1]\n",
      "\n",
      "Padded:\n",
      "\n",
      " the rock is destined to be the 21st century 's new `` conan '' and that he 's going to make a splash even greater than arnold schwarzenegger , <unk> van damme or steven segal . <s> <s> <s> <s>\n",
      "\n",
      "Un-padded:\n",
      "\n",
      " the rock is destined to be the 21st century 's new `` conan '' and that he 's going to make a splash even greater than arnold schwarzenegger , <unk> van damme or steven segal .\n"
     ]
    }
   ],
   "source": [
    "print(\"Examples:\\n\\n\", train_x[:3])\n",
    "print(\"\\nOriginal sequence lengths: \\t\", train_ns[:3])\n",
    "print(\"Target labels: \\t\\t\\t\", train_y[:3])\n",
    "print(\"\")\n",
    "print(\"Padded:\\n\\n\", \" \".join(ds.vocab.ids_to_words(train_x[0])))\n",
    "print(\"\\nUn-padded:\\n\\n\", \" \".join(ds.vocab.ids_to_words(train_x[0,:train_ns[0]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part (b): Implementing the Neural BOW Model\n",
    "\n",
    "In order to better manage the model code, we'll implement our BOW model in `models.py`. In particular, you should implement the following functions:\n",
    "\n",
    "- `embedding_layer(...)`: constructs an embedding layer\n",
    "- `BOW_encoder(...)`: constructs the encoder stack as described above\n",
    "- `softmax_output_layer(...)`: constructs a softmax output layer\n",
    "\n",
    "**Follow the instructions in the code (function docstrings and comments) carefully!**\n",
    "\n",
    "In particular, for unit tests to work, you shouldn't change (or add) any `tf.name_scope` or `tf.variable_scope` calls, and must name the variables exactly as documented. (Your model may work just fine, of course, but the test harness will throw all sorts of errors!)\n",
    "\n",
    "To aid debugging and readability, we've adopted a convention that TensorFlow tensors are represented by variables ending in an underscore, such as `W_embed_` or `train_op_`.\n",
    "\n",
    "You may find the following TensorFlow API functions useful:\n",
    "- [`tf.nn.embedding_lookup`](https://www.tensorflow.org/versions/master/api_docs/python/tf/nn/embedding_lookup)\n",
    "- [`tf.nn.sparse_softmax_cross_entropy_with_logits`](https://www.tensorflow.org/versions/master/api_docs/python/tf/nn/sparse_softmax_cross_entropy_with_logits)\n",
    "- [`tf.reduce_mean`](https://www.tensorflow.org/versions/master/api_docs/python/tf/reduce_mean) and [`tf.reduce_sum`](https://www.tensorflow.org/versions/master/api_docs/python/tf/reduce_sum)\n",
    "\n",
    "**Do your work in `models.py`.** When ready, run the cell below to run the unit tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_embedding_layer (models_test.TestLayerBuilders) ... ok\n",
      "test_softmax_output_layer (models_test.TestLayerBuilders) ... ok\n",
      "test_BOW_encoder (models_test.TestNeuralBOW) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.085s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "reload(models)\n",
    "utils.run_tests(models_test, [\"TestLayerBuilders\", \"TestNeuralBOW\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Neural Network (the hard way)\n",
    "\n",
    "We've implemented a wrapper function, `models.classifier_model_fn`, which uses the functions you wrote in **Part (b)** to build a model graph. It takes as input `features` and `labels` which contain input and target tensors, as well as `model` and `params` which configure the model. \n",
    "\n",
    "**Exercise (not graded):** Read through the code for `classifier_model_fn()` in `models.py`. Where is the code you wrote in Part (e) called? Where is the loss function set up, and what loss is used? How is the optimizer set up, and what options are available? What types of predictions are returned in the `predictions` dict?\n",
    "\n",
    "Using this function directly, we can write a simple training loop that directly feeds data from Python into a TensorFlow session. This will be fast, but very barebones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  800 examples, moving-average loss 0.33\n",
      "1,600 examples, moving-average loss 0.22\n",
      "2,400 examples, moving-average loss 0.19\n",
      "3,200 examples, moving-average loss 0.28\n",
      "4,000 examples, moving-average loss 0.40\n",
      "4,800 examples, moving-average loss 0.29\n",
      "5,600 examples, moving-average loss 0.30\n",
      "6,400 examples, moving-average loss 0.29\n",
      "Completed one epoch in 0:00:00\n"
     ]
    }
   ],
   "source": [
    "import models; reload(models)\n",
    "\n",
    "x, ns, y = train_x, train_ns, train_y\n",
    "batch_size = 32\n",
    "\n",
    "# Specify model hyperparameters as used by model_fn\n",
    "model_params = dict(V=ds.vocab.size, embed_dim=100, hidden_dims=[50, 25], num_classes=len(ds.target_names),\n",
    "                    encoder_type='bow',\n",
    "                    lr=0.1, optimizer='adagrad', beta=0)\n",
    "# model_params['embed_vecs'] = hands.W\n",
    "model_fn = models.classifier_model_fn\n",
    "\n",
    "total_batches = 0\n",
    "total_examples = 0\n",
    "total_loss = 0\n",
    "loss_ema = np.log(2)  # track exponential-moving-average of loss\n",
    "ema_decay = np.exp(-1/10)  # decay parameter for moving average = np.exp(-1/history_length)\n",
    "with tf.Graph().as_default(), tf.Session() as sess:\n",
    "    ##\n",
    "    # Construct the graph here. No session.run calls - just wiring up Tensors.\n",
    "    ##\n",
    "    # Add placeholders so we can feed in data.\n",
    "    x_ph_  = tf.placeholder(tf.int32, shape=[None, x.shape[1]])  # [batch_size, max_len]\n",
    "    ns_ph_ = tf.placeholder(tf.int32, shape=[None])              # [batch_size]\n",
    "    y_ph_  = tf.placeholder(tf.int32, shape=[None])              # [batch_size]\n",
    "    \n",
    "    # Construct the graph using model_fn\n",
    "    features = {\"ids\": x_ph_, \"ns\": ns_ph_}  # note that values are Tensors\n",
    "    estimator_spec = model_fn(features, labels=y_ph_, mode=tf.estimator.ModeKeys.TRAIN,\n",
    "                              params=model_params)\n",
    "    loss_     = estimator_spec.loss\n",
    "    train_op_ = estimator_spec.train_op\n",
    "    \n",
    "    ##\n",
    "    # Done constructing the graph, now we can make session.run calls.\n",
    "    ##\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Run a single epoch\n",
    "    t0 = time.time()\n",
    "    for (bx, bns, by) in utils.multi_batch_generator(batch_size, x, ns, y):\n",
    "        # feed NumPy arrays into the placeholder Tensors\n",
    "        feed_dict = {x_ph_: bx, ns_ph_: bns, y_ph_: by}\n",
    "        batch_loss, _ = sess.run([loss_, train_op_], feed_dict=feed_dict)\n",
    "        \n",
    "        # Compute some statistics\n",
    "        total_batches += 1\n",
    "        total_examples += len(bx)\n",
    "        total_loss += batch_loss * len(bx)  # re-scale, since batch loss is mean\n",
    "        # Compute moving average to smooth out noisy per-batch loss\n",
    "        loss_ema = ema_decay * loss_ema + (1 - ema_decay) * batch_loss\n",
    "        \n",
    "        if (total_batches % 25 == 0):\n",
    "            print(\"{:5,} examples, moving-average loss {:.2f}\".format(total_examples, \n",
    "                                                                      loss_ema))    \n",
    "    print(\"Completed one epoch in {:s}\".format(utils.pretty_timedelta(since=t0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part (c): Training a Neural Network with tf.Estimator\n",
    "\n",
    "As you see above, there's a lot of boilerplate involved with training a model - we need to instantiate the graph, manage a TensorFlow session, and manually feed data for each batch. This can get tedious, especially as we add support for checkpointing, saving models, and tracking statistics during training. (And as you build more complex models, these are _definitely_ things you'll want!) To streamline this process, we can use a high-level api like `tf.Estimator`.\n",
    "\n",
    "The Estimator API allows us to define custom models, then provides an `Estimator` object that exposes `train()`, `evaluate()`, and `predict()` functions in a similar interface as scikit-learn. Take a few minutes to skim through the main documentation:\n",
    "\n",
    "- [TensorFlow Estimator API](https://www.tensorflow.org/extend/estimators)\n",
    "- [Estimators in 'Effective TensorFlow'](https://github.com/vahidk/EffectiveTensorflow#tf_learn) (advanced)\n",
    "\n",
    "### Model Functions (model_fn)\n",
    "\n",
    "The Estimator API is a functional interface, built around the idea of a `model_fn`. A `model_fn` is just a function that follows a specific interface, and when called constructs a graph of TensorFlow variables and ops that constitutes your model. Here's an example of what one looks like:\n",
    "\n",
    "```python\n",
    "def my_model_fn(features, labels, mode, params):\n",
    "    x_ = features['x']\n",
    "    logits_ = my_network(x_, hidden_dims=params['hidden_dims'],\n",
    "                         foo=params['foo'], bar=params['bar'])\n",
    "    \n",
    "    predictions_dict = {\"max\": tf.argmax(logits_, 1)}\n",
    "    eval_metrics = {\"accuracy\": tf.metrics_accuracy(predictions_dict['max']}\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                                          predictions=predictions_dict)\n",
    "\n",
    "    loss_ = my_loss_fn(logits_)\n",
    "    return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                                      predictions=predictions_dict,\n",
    "                                      loss=loss_,\n",
    "                                      train_op=train_op_,\n",
    "                                      eval_metric_ops=eval_metrics)\n",
    "```\n",
    "You can read more about the arguments here: \n",
    "- [Constructing the model_fn](https://www.tensorflow.org/extend/estimators#constructing_the_model_fn)\n",
    "\n",
    "The Estimator API takes a pointer to this _function_, then calls it internally to instantiate your model in the appropriate context. This allows it to handle things like writing and restoring checkpoints automatically, as well as feeding data to the model during training and evaluation. \n",
    "\n",
    "### Input Functions (input_fn)\n",
    "\n",
    "Data feeding is handled by an `input_fn`, which takes the place of the placeholder variables and `feed_dict` we'd otherwise need. The `input_fn` is defined separately from the `model_fn`, and builds the part of the graph up to `features` and `labels`.\n",
    "\n",
    "We won't write our own `input_fn` in this assignment, but instead we can just use the existing `numpy_input_fn` implementation. This takes NumPy arrays as inputs, and creates an `input_fn` that will generate minibatches:\n",
    "\n",
    "```python\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "                    x={\"ids\": train_x, \"ns\": train_ns}, y=train_y,\n",
    "                    batch_size=32, num_epochs=20, shuffle=True\n",
    "                 )\n",
    "```\n",
    "\n",
    "You can read more about `input_fn`-s here: \n",
    "- [Building Input Functions with tf.Estimator](https://www.tensorflow.org/get_started/input_fn)\n",
    "\n",
    "**Note:** for this assignment, we'll use a patched version of `tf.estimator.inputs.numpy_input_fn` included with this assignment. This version allows us to seed the random number generator so that training data is shuffled but deterministic.\n",
    "\n",
    "### Building an Estimator\n",
    "\n",
    "With a `model_fn` and an `input_fn` in hand, we can now build and train an Estimator with just a couple of lines:\n",
    "\n",
    "```python\n",
    "model_params = dict(...)   # passed as 'params' to the model_fn\n",
    "model = tf.estimator.Estimator(model_fn=my_model_fn, \n",
    "                               params=model_params,\n",
    "                               model_dir=\"/tmp/my_model_checkpoints\")\n",
    "model.train(input_fn=train_input_fn)\n",
    "```\n",
    "\n",
    "The last line will kick off a train loop, ingesting data until the `input_fn` runs dry (20 epochs, for the one above). We can then evaluate on labeled data by calling `model.evaluate(input_fn=...)`, and run inference on unlabeled data by calling `model.predict(input_fn=...)` with appropriate `input_fn`-s.\n",
    "\n",
    "_**Note:** You might be wondering why TensorFlow adds all this boilerplate on top of the actual model. It doesn't seem necessary for small-scale experiments like this assignment, but as soon as you scale up to models that take hours, days, or even weeks to train, having robust checkpoint management, live dashboards, and distributed data queues really starts to pay off!_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part (d): Training and Evaluation\n",
    "\n",
    "The cell below defines some model params and sets up a checkpoint directory for TensorBoard.\n",
    "\n",
    "Use the following default parameters to start, as given below in `model_params`:\n",
    "```python\n",
    "embed_dim = 50\n",
    "hidden_dims = [25]  # single hidden layer\n",
    "optimizer = 'adagrad'\n",
    "lr = 0.1  # learning rate\n",
    "beta = 0.01  # L2 regularization\n",
    "```\n",
    "\n",
    "**Note:** Due to a bug in TensorFlow, if you re-use the same checkpoint directory (even after deleting the contents) it will sometimes fail to write the event data for TensorBoard. To work around this, the code below creates a new checkpoint directory each time with a name derived from the timestamp. You may want to delete these after a few runs, since they can take up ~35MB each. To do so just run:\n",
    "\n",
    "```sh\n",
    "# On command line\n",
    "rm -rfv /tmp/tf_bow_sst_*\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary (400,003 words) written to '/home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1853/metadata.tsv'\n",
      "Projector config written to /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1853/projector_config.pbtxt\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1853', '_tf_random_seed': 42, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ff14d455358>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "\n",
      "To view training (once it starts), run:\n",
      "\n",
      "    conda activate jsalt; tensorboard --logdir='/home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1853' --port 6006\n",
      "\n",
      "Then in your browser, open: http://localhost:6006\n"
     ]
    }
   ],
   "source": [
    "import models; reload(models)\n",
    "\n",
    "# Specify model hyperparameters as used by model_fn\n",
    "model_params = dict(V=ds.vocab.size, embed_dim=50, hidden_dims=[25], num_classes=len(ds.target_names),\n",
    "                    encoder_type='bow',\n",
    "                    lr=0.1, optimizer='adagrad', beta=0.001)\n",
    "\n",
    "def make_estimator(model_params):\n",
    "    if os.path.isdir(os.getcwd() + \"/tmp/\") is False: os.mkdir(os.getcwd() + \"/tmp/\")\n",
    "    checkpoint_dir = os.getcwd() + \"/tmp/tf_bow_sst_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "    os.mkdir(checkpoint_dir)\n",
    "    if os.path.isdir(checkpoint_dir):\n",
    "        shutil.rmtree(checkpoint_dir)\n",
    "\n",
    "    # Write vocabulary to file, so TensorBoard can label embeddings.\n",
    "    # creates checkpoint_dir/projector_config.pbtxt and checkpoint_dir/metadata.tsv\n",
    "    ds.vocab.write_projector_config(checkpoint_dir, \"Encoder/Embedding_Layer/W_embed\")\n",
    "\n",
    "    run_config = tf.estimator.RunConfig(tf_random_seed=42)\n",
    "    model = tf.estimator.Estimator(model_fn=models.classifier_model_fn, \n",
    "                                   params=model_params,\n",
    "                                   config=run_config, \n",
    "                                   model_dir=checkpoint_dir)\n",
    "    print(\"\")\n",
    "    print(\"To view training (once it starts), run:\\n\")\n",
    "    print(\"    conda activate jsalt; tensorboard --logdir='{:s}' --port 6006\".format(checkpoint_dir))\n",
    "    print(\"\\nThen in your browser, open: http://localhost:6006\")\n",
    "    \n",
    "    return model\n",
    "    \n",
    "model = make_estimator(model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the cell below to start training! If you run TensorBoard from the command line, you should see loss curves in the \"Scalars\" tab as training progresses. We've set it up to run an evaluation on the dev set every `train_params['eval_every']` epochs, and this should appear in the same tab as a blue line after a couple minutes.\n",
    "\n",
    "Using the default `model_params` above and the following training params, as given in `train_params` below:\n",
    "```python\n",
    "batch_size = 32\n",
    "total_epochs = 20\n",
    "eval_every = 2  # every 2 epochs, eval the dev set\n",
    "```\n",
    "Your model should train very quickly - 20 epochs in under two minutes on a single-core GCE instance.\n",
    "\n",
    "After 20 epochs, your loss curves should look something like this:\n",
    "\n",
    "![Loss curves](images/tensorboard_curves.png)\n",
    "\n",
    "Don't worry if they don't match exactly - colors may vary, and the red dot labeled \"eval\\_test\" won't appear until you run the evaluation cell below. There are also some other curves that you might see: \"global\\_step/sec\" is the number of minibatches per second that the model processes, and the \"enqueue\\_input/...\" plot has to do with the feeder queues that the Estimator API uses to stream data to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1853/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.7497469, step = 1\n",
      "INFO:tensorflow:global_step/sec: 338.453\n",
      "INFO:tensorflow:loss = 0.40973058, step = 101 (0.297 sec)\n",
      "INFO:tensorflow:global_step/sec: 401.767\n",
      "INFO:tensorflow:loss = 0.30776173, step = 201 (0.249 sec)\n",
      "INFO:tensorflow:global_step/sec: 446.361\n",
      "INFO:tensorflow:loss = 0.39250714, step = 301 (0.224 sec)\n",
      "INFO:tensorflow:global_step/sec: 418.215\n",
      "INFO:tensorflow:loss = 0.22943294, step = 401 (0.239 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 433 into /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1853/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.24158722.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-06-21-18:53:55\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1853/model.ckpt-433\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-06-21-18:53:55\n",
      "INFO:tensorflow:Saving dict for global step 433: accuracy = 0.6525229, cross_entropy_loss = 0.69553727, global_step = 433, loss = 0.75529903\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1853/model.ckpt-433\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 434 into /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1853/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.19264483, step = 434\n",
      "INFO:tensorflow:global_step/sec: 340.804\n",
      "INFO:tensorflow:loss = 0.3164161, step = 534 (0.295 sec)\n",
      "INFO:tensorflow:global_step/sec: 448.038\n",
      "INFO:tensorflow:loss = 0.2645305, step = 634 (0.223 sec)\n",
      "INFO:tensorflow:global_step/sec: 467.518\n",
      "INFO:tensorflow:loss = 0.20392925, step = 734 (0.214 sec)\n",
      "INFO:tensorflow:global_step/sec: 444.324\n",
      "INFO:tensorflow:loss = 0.08318909, step = 834 (0.225 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 866 into /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1853/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.08852738.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-06-21-18:53:58\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1853/model.ckpt-866\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-06-21-18:53:59\n",
      "INFO:tensorflow:Saving dict for global step 866: accuracy = 0.72477067, cross_entropy_loss = 0.82415116, global_step = 866, loss = 0.8854199\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1853/model.ckpt-866\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 867 into /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1853/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.08926945, step = 867\n",
      "INFO:tensorflow:global_step/sec: 337.735\n",
      "INFO:tensorflow:loss = 0.13623887, step = 967 (0.298 sec)\n",
      "INFO:tensorflow:global_step/sec: 440.713\n",
      "INFO:tensorflow:loss = 0.06977461, step = 1067 (0.227 sec)\n",
      "INFO:tensorflow:global_step/sec: 482.743\n",
      "INFO:tensorflow:loss = 0.07709251, step = 1167 (0.207 sec)\n",
      "INFO:tensorflow:global_step/sec: 484.597\n",
      "INFO:tensorflow:loss = 0.07506846, step = 1267 (0.206 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1299 into /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1853/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.09853332.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-06-21-18:54:01\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1853/model.ckpt-1299\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-06-21-18:54:02\n",
      "INFO:tensorflow:Saving dict for global step 1299: accuracy = 0.7522936, cross_entropy_loss = 0.9314676, global_step = 1299, loss = 0.99197453\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1853/model.ckpt-1299\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1300 into /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1853/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.05760318, step = 1300\n",
      "INFO:tensorflow:global_step/sec: 333.697\n",
      "INFO:tensorflow:loss = 0.058742557, step = 1400 (0.301 sec)\n",
      "INFO:tensorflow:global_step/sec: 445.041\n",
      "INFO:tensorflow:loss = 0.0520271, step = 1500 (0.225 sec)\n",
      "INFO:tensorflow:global_step/sec: 471.989\n",
      "INFO:tensorflow:loss = 0.06023944, step = 1600 (0.212 sec)\n",
      "INFO:tensorflow:global_step/sec: 442.884\n",
      "INFO:tensorflow:loss = 0.0604166, step = 1700 (0.226 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1732 into /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1853/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.075278476.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-06-21-18:54:04\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1853/model.ckpt-1732\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-06-21-18:54:05\n",
      "INFO:tensorflow:Saving dict for global step 1732: accuracy = 0.76490825, cross_entropy_loss = 1.1127965, global_step = 1732, loss = 1.1704742\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1853/model.ckpt-1732\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1733 into /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1853/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.045625668, step = 1733\n",
      "INFO:tensorflow:global_step/sec: 344.212\n",
      "INFO:tensorflow:loss = 0.04759734, step = 1833 (0.292 sec)\n",
      "INFO:tensorflow:global_step/sec: 418.414\n",
      "INFO:tensorflow:loss = 0.05038737, step = 1933 (0.239 sec)\n",
      "INFO:tensorflow:global_step/sec: 443.936\n",
      "INFO:tensorflow:loss = 0.049334783, step = 2033 (0.225 sec)\n",
      "INFO:tensorflow:global_step/sec: 445.974\n",
      "INFO:tensorflow:loss = 0.044510912, step = 2133 (0.224 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2165 into /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1853/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.050620522.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2018-06-21-18:54:07\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1853/model.ckpt-2165\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-06-21-18:54:08\n",
      "INFO:tensorflow:Saving dict for global step 2165: accuracy = 0.7614679, cross_entropy_loss = 1.1556056, global_step = 2165, loss = 1.209686\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1853/model.ckpt-2165\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 2166 into /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1853/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.054994747, step = 2166\n",
      "INFO:tensorflow:global_step/sec: 351.33\n",
      "INFO:tensorflow:loss = 0.04373187, step = 2266 (0.286 sec)\n",
      "INFO:tensorflow:global_step/sec: 439.631\n",
      "INFO:tensorflow:loss = 0.04358674, step = 2366 (0.228 sec)\n",
      "INFO:tensorflow:global_step/sec: 456.039\n",
      "INFO:tensorflow:loss = 0.044300344, step = 2466 (0.219 sec)\n",
      "INFO:tensorflow:global_step/sec: 437.305\n",
      "INFO:tensorflow:loss = 0.04218926, step = 2566 (0.229 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2598 into /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1853/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.04323519.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-06-21-18:54:11\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1853/model.ckpt-2598\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-06-21-18:54:11\n",
      "INFO:tensorflow:Saving dict for global step 2598: accuracy = 0.7603211, cross_entropy_loss = 1.2194932, global_step = 2598, loss = 1.2704462\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1853/model.ckpt-2598\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 2599 into /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1853/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.04120059, step = 2599\n",
      "INFO:tensorflow:global_step/sec: 368.502\n",
      "INFO:tensorflow:loss = 0.041264553, step = 2699 (0.273 sec)\n",
      "INFO:tensorflow:global_step/sec: 458.495\n",
      "INFO:tensorflow:loss = 0.04141654, step = 2799 (0.218 sec)\n",
      "INFO:tensorflow:global_step/sec: 468.471\n",
      "INFO:tensorflow:loss = 0.04179764, step = 2899 (0.214 sec)\n",
      "INFO:tensorflow:global_step/sec: 458.841\n",
      "INFO:tensorflow:loss = 0.04027719, step = 2999 (0.217 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3031 into /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1853/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.039297536.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-06-21-18:54:14\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1853/model.ckpt-3031\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-06-21-18:54:14\n",
      "INFO:tensorflow:Saving dict for global step 3031: accuracy = 0.7614679, cross_entropy_loss = 1.266386, global_step = 3031, loss = 1.3146322\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1853/model.ckpt-3031\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 3032 into /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1853/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.0388632, step = 3032\n",
      "INFO:tensorflow:global_step/sec: 383.777\n",
      "INFO:tensorflow:loss = 0.038743377, step = 3132 (0.262 sec)\n",
      "INFO:tensorflow:global_step/sec: 476.779\n",
      "INFO:tensorflow:loss = 0.03905474, step = 3232 (0.210 sec)\n",
      "INFO:tensorflow:global_step/sec: 455.174\n",
      "INFO:tensorflow:loss = 0.038934723, step = 3332 (0.220 sec)\n",
      "INFO:tensorflow:global_step/sec: 454.832\n",
      "INFO:tensorflow:loss = 0.038555846, step = 3432 (0.220 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3464 into /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1853/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.037036806.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-06-21-18:54:18\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1853/model.ckpt-3464\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-06-21-18:54:18\n",
      "INFO:tensorflow:Saving dict for global step 3464: accuracy = 0.7603211, cross_entropy_loss = 1.2834796, global_step = 3464, loss = 1.3293056\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1853/model.ckpt-3464\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 3465 into /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1853/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.036947697, step = 3465\n",
      "INFO:tensorflow:global_step/sec: 370.232\n",
      "INFO:tensorflow:loss = 0.036702015, step = 3565 (0.271 sec)\n",
      "INFO:tensorflow:global_step/sec: 470.929\n",
      "INFO:tensorflow:loss = 0.037188157, step = 3665 (0.212 sec)\n",
      "INFO:tensorflow:global_step/sec: 472.061\n",
      "INFO:tensorflow:loss = 0.0371641, step = 3765 (0.212 sec)\n",
      "INFO:tensorflow:global_step/sec: 449.154\n",
      "INFO:tensorflow:loss = 0.036983572, step = 3865 (0.223 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3897 into /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1853/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.035159424.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-06-21-18:54:21\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1853/model.ckpt-3897\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-06-21-18:54:21\n",
      "INFO:tensorflow:Saving dict for global step 3897: accuracy = 0.7603211, cross_entropy_loss = 1.2895437, global_step = 3897, loss = 1.3332193\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1853/model.ckpt-3897\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 3898 into /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1853/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.035271883, step = 3898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 344.273\n",
      "INFO:tensorflow:loss = 0.035007425, step = 3998 (0.292 sec)\n",
      "INFO:tensorflow:global_step/sec: 431.059\n",
      "INFO:tensorflow:loss = 0.035551317, step = 4098 (0.232 sec)\n",
      "INFO:tensorflow:global_step/sec: 438.278\n",
      "INFO:tensorflow:loss = 0.03541856, step = 4198 (0.228 sec)\n",
      "INFO:tensorflow:global_step/sec: 435.222\n",
      "INFO:tensorflow:loss = 0.03548367, step = 4298 (0.230 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4330 into /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1853/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.033432286.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-06-21-18:54:24\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1853/model.ckpt-4330\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-06-21-18:54:24\n",
      "INFO:tensorflow:Saving dict for global step 4330: accuracy = 0.7591743, cross_entropy_loss = 1.2927955, global_step = 4330, loss = 1.3345681\n"
     ]
    }
   ],
   "source": [
    "# Training params, just used in this cell for the input_fn-s\n",
    "# change total epochs to test!\n",
    "# train_params = dict(batch_size=32, total_epochs=2, eval_every=2)\n",
    "train_params = dict(batch_size=32, total_epochs=20, eval_every=2)\n",
    "assert(train_params['total_epochs'] % train_params['eval_every'] == 0)\n",
    "\n",
    "# Construct and train the model, saving checkpoints to the directory above.\n",
    "# Input function for training set batches\n",
    "# Do 'eval_every' epochs at once, followed by evaluating on the dev set.\n",
    "# NOTE: use patch_numpy_io.numpy_input_fn instead of tf.estimator.inputs.numpy_input_fn\n",
    "train_input_fn = patched_numpy_io.numpy_input_fn(\n",
    "                    x={\"ids\": train_x, \"ns\": train_ns}, y=train_y,\n",
    "                    batch_size=train_params['batch_size'], \n",
    "                    num_epochs=train_params['eval_every'], shuffle=True, seed=42,\n",
    "                 )\n",
    "\n",
    "# Input function for dev set batches. As above, but:\n",
    "# - Don't randomize order\n",
    "# - Iterate exactly once (one epoch)\n",
    "dev_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "                    x={\"ids\": dev_x, \"ns\": dev_ns}, y=dev_y,\n",
    "                    batch_size=128, num_epochs=1, shuffle=False\n",
    "                )\n",
    "\n",
    "for _ in range(train_params['total_epochs'] // train_params['eval_every']):\n",
    "    # Train for a few epochs, then evaluate on dev\n",
    "    model.train(input_fn=train_input_fn)\n",
    "    eval_metrics = model.evaluate(input_fn=dev_input_fn, name=\"dev\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Your Model\n",
    "\n",
    "To evaluate on the test set, we just need to construct another `input_fn`, then call `model.evaluate`. \n",
    "\n",
    "**1.)** Fill in the cell below, and run it to compute accuracy on the test set. With the default parameters, you should get accuracy around 77%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-06-21-18:54:25\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1853/model.ckpt-4330\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-06-21-18:54:25\n",
      "INFO:tensorflow:Saving dict for global step 4330: accuracy = 0.778693, cross_entropy_loss = 1.0988533, global_step = 4330, loss = 1.1398171\n",
      "Accuracy on test set: 77.87%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.778693,\n",
       " 'cross_entropy_loss': 1.0988533,\n",
       " 'global_step': 4330,\n",
       " 'loss': 1.1398171}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "# Code for Part (f).1\n",
    "test_input_fn = None  # replace with an input_fn, similar to dev_input_fn\n",
    "test_input_fn = tf.estimator.inputs.numpy_input_fn(              #--SOLUTION--\n",
    "                    x={\"ids\": test_x, \"ns\": test_ns}, y=test_y,  #--SOLUTION--\n",
    "                    batch_size=128, num_epochs=1, shuffle=False  #--SOLUTION--\n",
    "                )                                                #--SOLUTION--\n",
    "\n",
    "eval_metrics = None  # replace with result of model.evaluate(...)\n",
    "eval_metrics = model.evaluate(input_fn=test_input_fn, name=\"test\")  #--SOLUTION--\n",
    "\n",
    "#### END(YOUR CODE) ####\n",
    "print(\"Accuracy on test set: {:.02%}\".format(eval_metrics['accuracy']))\n",
    "eval_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also evaluate the old-fashioned way, by calling `model.predict(...)` and working with the predicted labels directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1853/model.ckpt-4330\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "Accuracy on test set: 77.87%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "predictions = list(model.predict(test_input_fn))  # list of dicts\n",
    "y_pred = [p['max'] for p in predictions]\n",
    "acc = accuracy_score(y_pred, test_y)\n",
    "print(\"Accuracy on test set: {:.02%}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part (e): GloVe vectors\n",
    "\n",
    "Let's see if we can improve performance by using pre-trained GloVe vectors. While the SST dataset contains only hundreds of thousands of tokens (and thousands of target labels), unsupervised methods like GloVe can be trained on billions of tokens of unlabeled text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train GloVe (summed):  (6920, 100)\n",
      "Dev GloVe (summed):    (872, 100)\n",
      "Test GloVe (summed):   (1821, 100)\n"
     ]
    }
   ],
   "source": [
    "def get_glove_vectors(X, hands):\n",
    "    output_mat = np.zeros((X.shape[0], hands.ndim))\n",
    "    for i, row in enumerate(X):\n",
    "        for token_id in row:\n",
    "            if token_id == 0:\n",
    "                continue  # skip padding\n",
    "            output_mat[i] += hands.W[token_id]\n",
    "    return output_mat.astype(np.float32)\n",
    "\n",
    "train_g = get_glove_vectors(train_x, hands); print(\"Train GloVe (summed): \", train_g.shape)\n",
    "dev_g   = get_glove_vectors(dev_x, hands);   print(\"Dev GloVe (summed):   \", dev_g.shape)\n",
    "test_g  = get_glove_vectors(test_x, hands);  print(\"Test GloVe (summed):  \", test_g.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary (400,003 words) written to '/home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1854/metadata.tsv'\n",
      "Projector config written to /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1854/projector_config.pbtxt\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1854', '_tf_random_seed': 42, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ff14b2f22b0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "\n",
      "To view training (once it starts), run:\n",
      "\n",
      "    conda activate jsalt; tensorboard --logdir='/home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1854' --port 6006\n",
      "\n",
      "Then in your browser, open: http://localhost:6006\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1854/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.17973106, step = 1\n",
      "INFO:tensorflow:global_step/sec: 664.387\n",
      "INFO:tensorflow:loss = 0.39417168, step = 101 (0.152 sec)\n",
      "INFO:tensorflow:global_step/sec: 911.063\n",
      "INFO:tensorflow:loss = 0.39617532, step = 201 (0.110 sec)\n",
      "INFO:tensorflow:global_step/sec: 965.22\n",
      "INFO:tensorflow:loss = 0.6655263, step = 301 (0.103 sec)\n",
      "INFO:tensorflow:global_step/sec: 1005\n",
      "INFO:tensorflow:loss = 0.39504087, step = 401 (0.099 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 433 into /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1854/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.3886985.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-06-21-18:54:28\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1854/model.ckpt-433\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-06-21-18:54:29\n",
      "INFO:tensorflow:Saving dict for global step 433: accuracy = 0.57568806, cross_entropy_loss = 0.79645246, global_step = 433, loss = 0.83026755\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1854/model.ckpt-433\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 434 into /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1854/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.28865397, step = 434\n",
      "INFO:tensorflow:global_step/sec: 655.371\n",
      "INFO:tensorflow:loss = 0.41060922, step = 534 (0.154 sec)\n",
      "INFO:tensorflow:global_step/sec: 924.367\n",
      "INFO:tensorflow:loss = 0.34312075, step = 634 (0.108 sec)\n",
      "INFO:tensorflow:global_step/sec: 895.71\n",
      "INFO:tensorflow:loss = 0.5381622, step = 734 (0.112 sec)\n",
      "INFO:tensorflow:global_step/sec: 987.13\n",
      "INFO:tensorflow:loss = 0.363922, step = 834 (0.101 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 866 into /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1854/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.41116908.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-06-21-18:54:30\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1854/model.ckpt-866\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-06-21-18:54:30\n",
      "INFO:tensorflow:Saving dict for global step 866: accuracy = 0.60779816, cross_entropy_loss = 0.7133295, global_step = 866, loss = 0.7467782\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1854/model.ckpt-866\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 867 into /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1854/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.310139, step = 867\n",
      "INFO:tensorflow:global_step/sec: 706.219\n",
      "INFO:tensorflow:loss = 0.3615631, step = 967 (0.143 sec)\n",
      "INFO:tensorflow:global_step/sec: 913.961\n",
      "INFO:tensorflow:loss = 0.32646847, step = 1067 (0.110 sec)\n",
      "INFO:tensorflow:global_step/sec: 907.257\n",
      "INFO:tensorflow:loss = 0.5373787, step = 1167 (0.110 sec)\n",
      "INFO:tensorflow:global_step/sec: 949.46\n",
      "INFO:tensorflow:loss = 0.35781524, step = 1267 (0.105 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1299 into /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1854/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.40350908.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-06-21-18:54:32\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1854/model.ckpt-1299\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-06-21-18:54:32\n",
      "INFO:tensorflow:Saving dict for global step 1299: accuracy = 0.64105505, cross_entropy_loss = 0.682116, global_step = 1299, loss = 0.71518767\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1854/model.ckpt-1299\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1300 into /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1854/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.34040928, step = 1300\n",
      "INFO:tensorflow:global_step/sec: 650.086\n",
      "INFO:tensorflow:loss = 0.35507414, step = 1400 (0.155 sec)\n",
      "INFO:tensorflow:global_step/sec: 923.652\n",
      "INFO:tensorflow:loss = 0.33077267, step = 1500 (0.108 sec)\n",
      "INFO:tensorflow:global_step/sec: 978.871\n",
      "INFO:tensorflow:loss = 0.4981526, step = 1600 (0.102 sec)\n",
      "INFO:tensorflow:global_step/sec: 962.122\n",
      "INFO:tensorflow:loss = 0.34903786, step = 1700 (0.104 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1732 into /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1854/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.40823105.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-06-21-18:54:34\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1854/model.ckpt-1732\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-06-21-18:54:34\n",
      "INFO:tensorflow:Saving dict for global step 1732: accuracy = 0.6525229, cross_entropy_loss = 0.6654984, global_step = 1732, loss = 0.69823974\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1854/model.ckpt-1732\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1733 into /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1854/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.30640823, step = 1733\n",
      "INFO:tensorflow:global_step/sec: 661.13\n",
      "INFO:tensorflow:loss = 0.35540166, step = 1833 (0.153 sec)\n",
      "INFO:tensorflow:global_step/sec: 895.895\n",
      "INFO:tensorflow:loss = 0.32820868, step = 1933 (0.112 sec)\n",
      "INFO:tensorflow:global_step/sec: 853.316\n",
      "INFO:tensorflow:loss = 0.47953492, step = 2033 (0.118 sec)\n",
      "INFO:tensorflow:global_step/sec: 835.492\n",
      "INFO:tensorflow:loss = 0.4065003, step = 2133 (0.120 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2165 into /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1854/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.41239542.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-06-21-18:54:36\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1854/model.ckpt-2165\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-06-21-18:54:36\n",
      "INFO:tensorflow:Saving dict for global step 2165: accuracy = 0.6662844, cross_entropy_loss = 0.6494364, global_step = 2165, loss = 0.6818698\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1854/model.ckpt-2165\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 2166 into /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1854/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.3053718, step = 2166\n",
      "INFO:tensorflow:global_step/sec: 715.87\n",
      "INFO:tensorflow:loss = 0.3004594, step = 2266 (0.143 sec)\n",
      "INFO:tensorflow:global_step/sec: 936.331\n",
      "INFO:tensorflow:loss = 0.32067585, step = 2366 (0.105 sec)\n",
      "INFO:tensorflow:global_step/sec: 891.855\n",
      "INFO:tensorflow:loss = 0.4650583, step = 2466 (0.112 sec)\n",
      "INFO:tensorflow:global_step/sec: 927.886\n",
      "INFO:tensorflow:loss = 0.33804998, step = 2566 (0.108 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2598 into /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1854/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.41069233.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-06-21-18:54:37\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1854/model.ckpt-2598\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-06-21-18:54:38\n",
      "INFO:tensorflow:Saving dict for global step 2598: accuracy = 0.67316514, cross_entropy_loss = 0.6447213, global_step = 2598, loss = 0.6768698\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1854/model.ckpt-2598\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 2599 into /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1854/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.3398022, step = 2599\n",
      "INFO:tensorflow:global_step/sec: 685.683\n",
      "INFO:tensorflow:loss = 0.3576757, step = 2699 (0.147 sec)\n",
      "INFO:tensorflow:global_step/sec: 928.765\n",
      "INFO:tensorflow:loss = 0.3246291, step = 2799 (0.108 sec)\n",
      "INFO:tensorflow:global_step/sec: 921.608\n",
      "INFO:tensorflow:loss = 0.45495373, step = 2899 (0.109 sec)\n",
      "INFO:tensorflow:global_step/sec: 1000.97\n",
      "INFO:tensorflow:loss = 0.3404469, step = 2999 (0.100 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3031 into /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1854/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.41573197.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-06-21-18:54:39\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1854/model.ckpt-3031\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-06-21-18:54:39\n",
      "INFO:tensorflow:Saving dict for global step 3031: accuracy = 0.6777523, cross_entropy_loss = 0.6386126, global_step = 3031, loss = 0.6704966\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1854/model.ckpt-3031\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 3032 into /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1854/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.30769017, step = 3032\n",
      "INFO:tensorflow:global_step/sec: 655.859\n",
      "INFO:tensorflow:loss = 0.35057002, step = 3132 (0.154 sec)\n",
      "INFO:tensorflow:global_step/sec: 904.075\n",
      "INFO:tensorflow:loss = 0.31994918, step = 3232 (0.110 sec)\n",
      "INFO:tensorflow:global_step/sec: 920.981\n",
      "INFO:tensorflow:loss = 0.44573444, step = 3332 (0.109 sec)\n",
      "INFO:tensorflow:global_step/sec: 898.783\n",
      "INFO:tensorflow:loss = 0.3410883, step = 3432 (0.111 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3464 into /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1854/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.41188407.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-06-21-18:54:41\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1854/model.ckpt-3464\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-06-21-18:54:41\n",
      "INFO:tensorflow:Saving dict for global step 3464: accuracy = 0.6788991, cross_entropy_loss = 0.6333392, global_step = 3464, loss = 0.66497695\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1854/model.ckpt-3464\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 3465 into /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1854/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.38253593, step = 3465\n",
      "INFO:tensorflow:global_step/sec: 703.457\n",
      "INFO:tensorflow:loss = 0.3475297, step = 3565 (0.144 sec)\n",
      "INFO:tensorflow:global_step/sec: 976.005\n",
      "INFO:tensorflow:loss = 0.3192381, step = 3665 (0.102 sec)\n",
      "INFO:tensorflow:global_step/sec: 1008.87\n",
      "INFO:tensorflow:loss = 0.4342333, step = 3765 (0.099 sec)\n",
      "INFO:tensorflow:global_step/sec: 988.267\n",
      "INFO:tensorflow:loss = 0.34238815, step = 3865 (0.102 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3897 into /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1854/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.4089056.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-06-21-18:54:42\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1854/model.ckpt-3897\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-06-21-18:54:43\n",
      "INFO:tensorflow:Saving dict for global step 3897: accuracy = 0.6857798, cross_entropy_loss = 0.62961435, global_step = 3897, loss = 0.66102445\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1854/model.ckpt-3897\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 3898 into /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1854/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.26721704, step = 3898\n",
      "INFO:tensorflow:global_step/sec: 686.285\n",
      "INFO:tensorflow:loss = 0.34138396, step = 3998 (0.147 sec)\n",
      "INFO:tensorflow:global_step/sec: 958.356\n",
      "INFO:tensorflow:loss = 0.31323367, step = 4098 (0.104 sec)\n",
      "INFO:tensorflow:global_step/sec: 946.142\n",
      "INFO:tensorflow:loss = 0.4098095, step = 4198 (0.106 sec)\n",
      "INFO:tensorflow:global_step/sec: 1000.66\n",
      "INFO:tensorflow:loss = 0.3456114, step = 4298 (0.100 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4330 into /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1854/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.40940842.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-06-21-18:54:44\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1854/model.ckpt-4330\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-06-21-18:54:44\n",
      "INFO:tensorflow:Saving dict for global step 4330: accuracy = 0.6869266, cross_entropy_loss = 0.62709963, global_step = 4330, loss = 0.6582966\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-06-21-18:54:45\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/iftenney/jsalt/summerschool/assignment/classifier-dev/tmp/tf_bow_sst_20180621-1854/model.ckpt-4330\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-06-21-18:54:46\n",
      "INFO:tensorflow:Saving dict for global step 4330: accuracy = 0.66886324, cross_entropy_loss = 0.6390379, global_step = 4330, loss = 0.670235\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.66886324,\n",
       " 'cross_entropy_loss': 0.6390379,\n",
       " 'global_step': 4330,\n",
       " 'loss': 0.670235}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(models)\n",
    "\n",
    "def train_and_evaluate_glove(model, train_params):\n",
    "    train_input_fn = patched_numpy_io.numpy_input_fn(\n",
    "                        x={\"x\": train_g}, y=train_y,\n",
    "                        batch_size=train_params['batch_size'], \n",
    "                        num_epochs=train_params['eval_every'], shuffle=True, seed=42,\n",
    "                     )\n",
    "\n",
    "    # Input function for dev set batches. As above, but:\n",
    "    # - Don't randomize order\n",
    "    # - Iterate exactly once (one epoch)\n",
    "    dev_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "                        x={\"x\": dev_g}, y=dev_y,\n",
    "                        batch_size=128, num_epochs=1, shuffle=False\n",
    "                    )\n",
    "    \n",
    "\n",
    "    for _ in range(train_params['total_epochs'] // train_params['eval_every']):\n",
    "        # Train for a few epochs, then evaluate on dev\n",
    "        model.train(input_fn=train_input_fn)\n",
    "        eval_metrics = model.evaluate(input_fn=dev_input_fn, name=\"dev\")\n",
    "        \n",
    "    test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "                        x={\"x\": test_g}, y=test_y,\n",
    "                        batch_size=128, num_epochs=1, shuffle=False\n",
    "                    )\n",
    "    return model.evaluate(input_fn=test_input_fn, name=\"test\")\n",
    "\n",
    "\n",
    "model_params_glove = copy.deepcopy(model_params)\n",
    "model_params_glove['hidden_dims'] = [25, 10]\n",
    "model_params_glove['lr'] = 0.01\n",
    "# model_params_glove['optimizer'] = 'sgd'\n",
    "# model_params_glove['beta'] = 0.05\n",
    "model_params_glove['encoder_type'] = 'mlp'  # directly classify from vectors\n",
    "model = make_estimator(model_params_glove)\n",
    "\n",
    "train_params = dict(batch_size=32, total_epochs=20, eval_every=2)\n",
    "train_and_evaluate_glove(model, train_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, since we don't need an explicit embedding layer (so long as the GloVe vectors are taken as fixed), we can even train a simple linear model over these vectors, which will perform quite well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 76.06%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(train_g, train_y)\n",
    "y_pred = lr.predict(test_g)\n",
    "\n",
    "# predictions = list(model.predict(test_input_fn))  # list of dicts\n",
    "# y_pred = [p['max'] for p in predictions]\n",
    "acc = accuracy_score(y_pred, test_y)\n",
    "print(\"Accuracy on test set: {:.02%}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part (f): ELMo Contextual Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import elmo_runner; reload(elmo_runner)\n",
    "from tqdm import tqdm\n",
    "\n",
    "def tokens_to_elmo(tokens_input):\n",
    "    gen = elmo_runner.process_elmo(tokens_input, bow=True)\n",
    "    t = np.vstack(gen)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/6920 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using /tmp/tfhub_modules to cache modules.\n",
      "INFO:tensorflow:Initialize variable module/aggregation/scaling:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with aggregation/scaling\n",
      "INFO:tensorflow:Initialize variable module/aggregation/weights:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with aggregation/weights\n",
      "INFO:tensorflow:Initialize variable module/bilm/CNN/W_cnn_0:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN/W_cnn_0\n",
      "INFO:tensorflow:Initialize variable module/bilm/CNN/W_cnn_1:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN/W_cnn_1\n",
      "INFO:tensorflow:Initialize variable module/bilm/CNN/W_cnn_2:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN/W_cnn_2\n",
      "INFO:tensorflow:Initialize variable module/bilm/CNN/W_cnn_3:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN/W_cnn_3\n",
      "INFO:tensorflow:Initialize variable module/bilm/CNN/W_cnn_4:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN/W_cnn_4\n",
      "INFO:tensorflow:Initialize variable module/bilm/CNN/W_cnn_5:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN/W_cnn_5\n",
      "INFO:tensorflow:Initialize variable module/bilm/CNN/W_cnn_6:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN/W_cnn_6\n",
      "INFO:tensorflow:Initialize variable module/bilm/CNN/b_cnn_0:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN/b_cnn_0\n",
      "INFO:tensorflow:Initialize variable module/bilm/CNN/b_cnn_1:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN/b_cnn_1\n",
      "INFO:tensorflow:Initialize variable module/bilm/CNN/b_cnn_2:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN/b_cnn_2\n",
      "INFO:tensorflow:Initialize variable module/bilm/CNN/b_cnn_3:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN/b_cnn_3\n",
      "INFO:tensorflow:Initialize variable module/bilm/CNN/b_cnn_4:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN/b_cnn_4\n",
      "INFO:tensorflow:Initialize variable module/bilm/CNN/b_cnn_5:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN/b_cnn_5\n",
      "INFO:tensorflow:Initialize variable module/bilm/CNN/b_cnn_6:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN/b_cnn_6\n",
      "INFO:tensorflow:Initialize variable module/bilm/CNN_high_0/W_carry:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN_high_0/W_carry\n",
      "INFO:tensorflow:Initialize variable module/bilm/CNN_high_0/W_transform:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN_high_0/W_transform\n",
      "INFO:tensorflow:Initialize variable module/bilm/CNN_high_0/b_carry:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN_high_0/b_carry\n",
      "INFO:tensorflow:Initialize variable module/bilm/CNN_high_0/b_transform:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN_high_0/b_transform\n",
      "INFO:tensorflow:Initialize variable module/bilm/CNN_high_1/W_carry:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN_high_1/W_carry\n",
      "INFO:tensorflow:Initialize variable module/bilm/CNN_high_1/W_transform:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN_high_1/W_transform\n",
      "INFO:tensorflow:Initialize variable module/bilm/CNN_high_1/b_carry:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN_high_1/b_carry\n",
      "INFO:tensorflow:Initialize variable module/bilm/CNN_high_1/b_transform:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN_high_1/b_transform\n",
      "INFO:tensorflow:Initialize variable module/bilm/CNN_proj/W_proj:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN_proj/W_proj\n",
      "INFO:tensorflow:Initialize variable module/bilm/CNN_proj/b_proj:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/CNN_proj/b_proj\n",
      "INFO:tensorflow:Initialize variable module/bilm/RNN_0/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/bias:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/RNN_0/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/bias\n",
      "INFO:tensorflow:Initialize variable module/bilm/RNN_0/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/kernel:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/RNN_0/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/kernel\n",
      "INFO:tensorflow:Initialize variable module/bilm/RNN_0/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/projection/kernel:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/RNN_0/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/projection/kernel\n",
      "INFO:tensorflow:Initialize variable module/bilm/RNN_0/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/bias:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/RNN_0/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/bias\n",
      "INFO:tensorflow:Initialize variable module/bilm/RNN_0/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/kernel:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/RNN_0/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/kernel\n",
      "INFO:tensorflow:Initialize variable module/bilm/RNN_0/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/projection/kernel:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/RNN_0/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/projection/kernel\n",
      "INFO:tensorflow:Initialize variable module/bilm/RNN_1/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/bias:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/RNN_1/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/bias\n",
      "INFO:tensorflow:Initialize variable module/bilm/RNN_1/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/kernel:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/RNN_1/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/kernel\n",
      "INFO:tensorflow:Initialize variable module/bilm/RNN_1/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/projection/kernel:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/RNN_1/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/projection/kernel\n",
      "INFO:tensorflow:Initialize variable module/bilm/RNN_1/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/bias:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/RNN_1/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/bias\n",
      "INFO:tensorflow:Initialize variable module/bilm/RNN_1/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/kernel:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/RNN_1/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/kernel\n",
      "INFO:tensorflow:Initialize variable module/bilm/RNN_1/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/projection/kernel:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/RNN_1/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/projection/kernel\n",
      "INFO:tensorflow:Initialize variable module/bilm/char_embed:0 from checkpoint b'/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d/variables/variables' with bilm/char_embed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 332/6920 [02:27<48:49,  2.25it/s] "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-7fae28f0f752>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_tok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_token_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_elmo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens_to_elmo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_tok\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_elmo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# np.save(os.path.join(os.getcwd(), \"sst.train.elmo_bow.npy\"), train_elmo)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-f2a0710ff7cc>\u001b[0m in \u001b[0;36mtokens_to_elmo\u001b[0;34m(tokens_input)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtokens_to_elmo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mgen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melmo_runner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_elmo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mvstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \"\"\"\n\u001b[0;32m--> 234\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_m\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \"\"\"\n\u001b[0;32m--> 234\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_m\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jsalt/summerschool/assignment/classifier-dev/elmo_runner.py\u001b[0m in \u001b[0;36mprocess_elmo\u001b[0;34m(tokens_lists, bow)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens_lists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0mfeeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mtokens_input\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength_input\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                 \u001b[0memb_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeeds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0memb_np\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_tok, train_y = ds.as_token_list('train', root_only=True)\n",
    "train_elmo = tokens_to_elmo(tqdm(train_tok))\n",
    "\n",
    "print(train_elmo.shape)\n",
    "# np.save(os.path.join(os.getcwd(), \"sst.train.elmo_bow.npy\"), train_elmo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_tok, dev_y = ds.as_token_list('dev', root_only=True)\n",
    "dev_elmo = tokens_to_elmo(tqdm(dev_tok))\n",
    "\n",
    "print(dev_elmo.shape)\n",
    "np.save(os.path.join(os.getcwd(), \"sst.dev.elmo_bow.npy\"), dev_elmo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tok, test_y = ds.as_token_list('test', root_only=True)\n",
    "test_elmo = tokens_to_elmo(tqdm(test_tok))\n",
    "\n",
    "print(test_elmo.shape)\n",
    "np.save(os.path.join(os.getcwd(), \"sst.test.elmo_bow.npy\"), test_elmo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(elmo_runner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "with tf.Graph().as_default():\n",
    "    elmo = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=False)\n",
    "    tokens_input = [[\"the\", \"cat\", \"is\", \"on\", \"the\", \"mat\"],\n",
    "                    [\"dogs\", \"are\", \"in\", \"the\", \"fog\", \"\"]]\n",
    "    tokens_length = [6, 5]\n",
    "    embeddings = elmo(\n",
    "        inputs={\n",
    "            \"tokens\": tokens_input,\n",
    "            \"sequence_len\": tokens_length\n",
    "        },\n",
    "        signature=\"tokens\",\n",
    "        as_dict=True)[\"elmo\"]\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.tables_initializer())\n",
    "        emb_np = sess.run(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
