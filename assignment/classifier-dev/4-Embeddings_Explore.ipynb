{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings - Exploration\n",
    "\n",
    "In this part of the assignment, we'll explore a few properties of word embeddings. We'll use pre-trained GloVe ([Pennington et al. 2013](https://nlp.stanford.edu/pubs/glove.pdf)) embeddings, and evaluate on the analogy task described in ([Mikolov et al. 2013](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)).\n",
    "\n",
    "You may also want to take a look at the [Embedding Co-occurrence and Visualization notebook](Embeddings_SVD_Viz.ipynb); this notebook will build somewhat on that material."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install a few python packages using pip\n",
    "from common import utils\n",
    "utils.require_package(\"wget\")      # for fetching dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard python helper libraries.\n",
    "import os, sys, re, json, time\n",
    "import itertools, collections\n",
    "from importlib import reload\n",
    "from IPython.display import display\n",
    "\n",
    "# NumPy and SciPy for matrix ops\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "\n",
    "# NLTK for NLP utils\n",
    "import nltk\n",
    "\n",
    "# Helper libraries\n",
    "from common import utils, vocabulary, tf_embed_viz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fits like a GloVe\n",
    "\n",
    "Word embeddings take a long time to train - since the goal is to provide a good representation for as many words as possible, generating good embeddings often requires making several passes over a very large corpus. \n",
    "\n",
    "Fortunately, it's possible to learn fairly general embeddings from large corpora that are useful for many downstream tasks. We'll use the GloVe vectors available at https://nlp.stanford.edu/projects/glove/ - specifically, a set trained with a vocabulary of 400,000 on a corpus of 6B tokens from Wikipedia and Gigaword.\n",
    "\n",
    "The vectors are distributed as a (very) large text file, with one word per line followed by its vector:\n",
    "```\n",
    "the -0.038194 -0.24487 0.72812 -0.39961 0.083172 0.043953 -0.39141 0.3344 -0.57545 0.087459\n",
    "```\n",
    "\n",
    "We've implemented a helper class, `Hands` in `glove_helper.py`, that will parse these files in a memory efficient manner and provide a wrapper object over a NumPy array containing the actual vectors. \n",
    "\n",
    "Run the cell below; the first time, it will download an ~800 MB file to the `data/` directory. **_Please do not check this in to git!_** Also be sure that you have at lease 2 GB of free memory on your workstation; Python is not very efficient, so these embeddings can be significantly larger when loaded into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glove_helper; reload(glove_helper)\n",
    "\n",
    "hands = glove_helper.Hands(ndim=100)  # 50, 100, 200, 300 dim are available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`hands` has a few properties and methods that might be useful:\n",
    "- `hands.vocab` is a `vocabulary.Vocabulary` object that manages the set of available words\n",
    "- `hands.W` is a matrix of shape $|V| \\times d$ containing the actual vectors, one per row. Row indices are as given by `hands.vocab.word_to_id[word]`.\n",
    "- `hands.get_vector(word)` returns the vector for a word (passed as a string).\n",
    "\n",
    "Note that we let $|V| = $`hands.W.shape[0]`, which in addition to the actual words includes three special tokens: `<s>` (begin sentence), `</s>` (end sentence), and `<unk>` (unknown word)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part (a): Nearest Neighbors\n",
    "\n",
    "### Cosine Similarity\n",
    "\n",
    "To measure the similarity of two words, we'll use the [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) between their representation vectors:\n",
    "\n",
    "$$ D^{cos}_{ij} = \\frac{v_i^T v_j}{||v_i||\\ ||v_j||}$$\n",
    "\n",
    "*Note that this is called cosine similarity because $D^{cos}_{ij} = \\cos(\\theta_{ij})$, where $\\theta_{ij}$ is the angle between the two vectors.*\n",
    "\n",
    "## Part (a) Questions\n",
    "\n",
    "1. In `vector_math.py`, implement the `find_nn_cos(...)` function. Read the docstring _carefully_ - it describes what you should return. *Hint: use NumPy functions instead of a `for` loop.*\n",
    "<p>\n",
    "2. Use the `show_nns(...)` function below to find the nearest neighbors for the words `\"bank\"`, `\"plane\"`, and `\"flies\"`. Are the neighbors dominated by one sense of these words or another? Is there evidence that the vector encodes meaning of the other senses as well?\n",
    "<p>\n",
    "3. Like `word2vec`, GloVe constructs representations by summarizing word-word coocurrence statistics. Use `show_nns(...)` to find the neighbors of `\"green\"` and `\"celadon\"`, and `\"orange\"` and `\"ochre\"`. Explain what you find in terms of the distributional hypothesis and the grounding problem. Do the vectors for `\"ochre\"` and `\"celadon\"` appear to encode a notion of color? What do they represent, instead?\n",
    "\n",
    "_(Recall that the Distributional Hypothesis is the idea that \"you shall know a word by the company it keeps\" (Firth, 1957) - that meaning is derived from the context in which a word is used. Grounding refers to the meaning of language in terms of external concepts, such as real-world entities or physical characteristics.)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vector_math; reload(vector_math)\n",
    "\n",
    "def show_nns(hands, word, k=10):\n",
    "    \"\"\"Helper function to print neighbors of a given word.\"\"\"\n",
    "    word = word.lower()\n",
    "    print(\"Nearest neighbors for '{:s}'\".format(word))\n",
    "    v = hands.get_vector(word)\n",
    "    for i, sim in zip(*vector_math.find_nn_cos(v, hands.W, k)):\n",
    "        target_word = hands.vocab.id_to_word[i]\n",
    "        print(\"{:.03f} : '{:s}'\".format(sim, target_word))\n",
    "    print(\"\")\n",
    "    \n",
    "show_nns(hands, \"the\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbors for 'flies'\n",
      "1.000 : 'flies'\n",
      "0.741 : 'fly'\n",
      "0.644 : 'flying'\n",
      "0.634 : 'insects'\n",
      "0.632 : 'flew'\n",
      "0.618 : 'butterflies'\n",
      "0.614 : 'moths'\n",
      "0.609 : 'moth'\n",
      "0.581 : 'planes'\n",
      "0.576 : 'plane'\n",
      "\n",
      "Nearest neighbors for 'plane'\n",
      "1.000 : 'plane'\n",
      "0.865 : 'airplane'\n",
      "0.822 : 'flight'\n",
      "0.820 : 'jet'\n",
      "0.808 : 'crashed'\n",
      "0.793 : 'crash'\n",
      "0.775 : 'planes'\n",
      "0.773 : 'helicopter'\n",
      "0.758 : 'airliner'\n",
      "0.753 : 'flying'\n",
      "\n",
      "Nearest neighbors for 'bank'\n",
      "1.000 : 'bank'\n",
      "0.806 : 'banks'\n",
      "0.753 : 'banking'\n",
      "0.704 : 'credit'\n",
      "0.694 : 'investment'\n",
      "0.678 : 'financial'\n",
      "0.669 : 'securities'\n",
      "0.665 : 'lending'\n",
      "0.648 : 'funds'\n",
      "0.648 : 'ubs'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "# Code for Part (a).2\n",
    "show_nns(hands, \"flies\")   #--SOLUTION--\n",
    "show_nns(hands, \"plane\")   #--SOLUTION--\n",
    "show_nns(hands, \"bank\")    #--SOLUTION--\n",
    "\n",
    "\n",
    "#### END(YOUR CODE) ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbors for 'orange'\n",
      "1.000 : 'orange'\n",
      "0.736 : 'yellow'\n",
      "0.714 : 'red'\n",
      "0.712 : 'blue'\n",
      "0.711 : 'green'\n",
      "0.678 : 'pink'\n",
      "0.677 : 'purple'\n",
      "0.671 : 'black'\n",
      "0.665 : 'colored'\n",
      "0.625 : 'lemon'\n",
      "\n",
      "Nearest neighbors for 'ochre'\n",
      "1.000 : 'ochre'\n",
      "0.687 : 'pigment'\n",
      "0.677 : 'reddish'\n",
      "0.674 : 'ocher'\n",
      "0.662 : 'coloured'\n",
      "0.658 : 'greenish'\n",
      "0.648 : 'magenta'\n",
      "0.634 : 'pigments'\n",
      "0.632 : 'yellowish'\n",
      "0.629 : 'mottled'\n",
      "\n",
      "Nearest neighbors for 'green'\n",
      "1.000 : 'green'\n",
      "0.820 : 'red'\n",
      "0.787 : 'blue'\n",
      "0.781 : 'brown'\n",
      "0.771 : 'yellow'\n",
      "0.762 : 'white'\n",
      "0.749 : 'gray'\n",
      "0.733 : 'black'\n",
      "0.729 : 'pink'\n",
      "0.728 : 'purple'\n",
      "\n",
      "Nearest neighbors for 'celadon'\n",
      "1.000 : 'celadon'\n",
      "0.620 : 'faience'\n",
      "0.602 : 'porcelains'\n",
      "0.594 : 'majolica'\n",
      "0.591 : 'ocher'\n",
      "0.585 : 'blue-and-white'\n",
      "0.575 : 'glazes'\n",
      "0.563 : 'unglazed'\n",
      "0.558 : 'porcelain'\n",
      "0.549 : 'steatite'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "# Code for Part (a).3\n",
    "show_nns(hands, \"orange\")  #--SOLUTION--\n",
    "show_nns(hands, \"ochre\")   #--SOLUTION--\n",
    "show_nns(hands, \"green\")   #--SOLUTION--\n",
    "show_nns(hands, \"celadon\") #--SOLUTION--\n",
    "\n",
    "\n",
    "#### END(YOUR CODE) ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part (b): Linear Analogies\n",
    "\n",
    "In this part, you'll implement the word analogy task described in Section 4 of ([Mikolov et al. 2013](https://arxiv.org/pdf/1301.3781.pdf)), and discussed in section 4.8 and 4.11 of the async.\n",
    "\n",
    "1. In `vector_math.py`, implement the `analogy(...)` function. (*Hint: this should be a very short function, given what you've already written above.*)\n",
    "<p>\n",
    "2. Evaluate a few analogies using the `show_analogy(...)` function below. In particular, find at least one analogy that tests each of the following relationships, and that the model gets right:<ul>\n",
    "<li> Singular / plural\n",
    "<li> Superlatives\n",
    "<li> Verb tense\n",
    "<li> Country / capital\n",
    "</ul>\n",
    "(See Table 1 of ([Mikolov et al. 2013](https://arxiv.org/pdf/1301.3781.pdf)) for a few ideas)\n",
    "<p>\n",
    "3. Evaluate the following analogies:\n",
    "<ul>\n",
    "<li> `\"lizard\" is to \"reptile\" as \"dog\" is to ____`\n",
    "<li> `\"finger\" is to  \"hand\"   as \"toe\" is to ____`\n",
    "</ul>\n",
    "What types of relations do these test? (*Hint: think back to WordNet, and things that end in -nymy.*) Does our approach of linear analogies work well here? What assumption is violated by these sorts of relationships? (*Hint: what if we reversed the order, and tested \"reptile\" is to \"lizard\", and so on?*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'king' is to 'queen' as 'man' is to ___\n",
      "0.804 : 'woman'\n",
      "0.779 : 'man'\n",
      "0.735 : 'girl'\n",
      "0.682 : 'she'\n",
      "0.659 : 'her'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import vector_math; reload(vector_math)\n",
    "\n",
    "def show_analogy(hands, a, b, c, k=5):\n",
    "    \"\"\"Compute and print a vector analogy.\"\"\"\n",
    "    a, b, c = a.lower(), b.lower(), c.lower()\n",
    "    va = hands.get_vector(a)\n",
    "    vb = hands.get_vector(b)\n",
    "    vc = hands.get_vector(c)\n",
    "    print(\"'{a:s}' is to '{b:s}' as '{c:s}' is to ___\".format(**locals()))\n",
    "    for i, sim in zip(*vector_math.analogy(va, vb, vc, hands.W, k)):\n",
    "        target_word = hands.vocab.id_to_word[i]\n",
    "        print(\"{:.03f} : '{:s}'\".format(sim, target_word))\n",
    "    print(\"\")\n",
    "    \n",
    "show_analogy(hands, \"king\", \"queen\", \"man\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'mouse' is to 'mice' as 'dog' is to ___\n",
      "0.808 : 'dogs'\n",
      "0.729 : 'rats'\n",
      "0.715 : 'dog'\n",
      "0.704 : 'cats'\n",
      "0.689 : 'mice'\n",
      "\n",
      "'fast' is to 'fastest' as 'slow' is to ___\n",
      "0.855 : 'fastest'\n",
      "0.744 : 'slowest'\n",
      "0.640 : 'fourth'\n",
      "0.640 : 'slower'\n",
      "0.629 : 'slow'\n",
      "\n",
      "'russia' is to 'moscow' as 'greece' is to ___\n",
      "0.797 : 'athens'\n",
      "0.753 : 'greece'\n",
      "0.701 : 'thessaloniki'\n",
      "0.698 : 'istanbul'\n",
      "0.668 : 'moscow'\n",
      "\n",
      "'work' is to 'works' as 'speak' is to ___\n",
      "0.851 : 'speak'\n",
      "0.749 : 'speaks'\n",
      "0.719 : 'spoken'\n",
      "0.627 : 'spoke'\n",
      "0.621 : 'speaking'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "# Code for Part (b).2\n",
    "show_analogy(hands, \"mouse\", \"mice\", \"dog\")        #--SOLUTION--\n",
    "show_analogy(hands, \"fast\", \"fastest\", \"slow\")     #--SOLUTION--\n",
    "show_analogy(hands, \"russia\", \"moscow\", \"greece\")  #--SOLUTION--\n",
    "show_analogy(hands, \"work\", \"works\", \"speak\")      #--SOLUTION--\n",
    "\n",
    "\n",
    "#### END(YOUR CODE) ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'lizard' is to 'reptile' as 'dog' is to ___\n",
      "0.768 : 'dog'\n",
      "0.682 : 'dogs'\n",
      "0.662 : 'pet'\n",
      "0.635 : 'puppy'\n",
      "0.629 : 'animal'\n",
      "\n",
      "'finger' is to 'hand' as 'toe' is to ___\n",
      "0.776 : 'toe'\n",
      "0.625 : 'hand'\n",
      "0.581 : 'shoes'\n",
      "0.567 : 'back'\n",
      "0.566 : 'hands'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "# Code for Part (b).3\n",
    "show_analogy(hands, \"lizard\", \"reptile\", \"dog\")  #--SOLUTION--\n",
    "show_analogy(hands, \"finger\", \"hand\", \"toe\")     #--SOLUTION--\n",
    "\n",
    "\n",
    "#### END(YOUR CODE) ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
